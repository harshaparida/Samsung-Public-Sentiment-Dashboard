{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b2c193-39c5-4ede-825e-90fdd9cb62c6",
   "metadata": {},
   "source": [
    "## Real time Public sentiment analysis for Samsung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3a689-8675-4620-a5c3-50b2083d523a",
   "metadata": {},
   "source": [
    "### Installing the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26050096-795d-4970-a1d4-2b105aa82a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\harsha\\anaconda3\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\harsha\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\harsha\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from tweepy) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264a7e28-0351-4836-9b12-b4b505226b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Harsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac0128-a633-4327-a2dd-27e37b4e670a",
   "metadata": {},
   "source": [
    "This script fetches the latest 100 tweets about \"Samsung\" using the Twitter API (via Tweepy), filters out retweets and non-English tweets, and analyzes the sentiment of each tweet using NLTK’s VADER sentiment analyzer. It labels each tweet as Positive, Negative, or Neutral based on its sentiment score. Then, it stores the tweet text, date, and sentiment in a table and saves the results to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a06dc4-3833-4267-b73a-df68299a4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Harsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 100 Samsung tweets with sentiment labels to samsung_sentiment_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# --- Your Bearer Token from Developer Portal ---\n",
    "BEARER_TOKEN = \"token\"\n",
    "\n",
    "# Initialize client\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Define query and tweet fetch function\n",
    "query = \"Samsung -is:retweet lang:en\"\n",
    "tweets = client.search_recent_tweets(query=query, tweet_fields=['created_at', 'text'], max_results=100)\n",
    "\n",
    "# Analyze sentiment\n",
    "data = []\n",
    "for tweet in tweets.data:\n",
    "    text = tweet.text\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    if sentiment_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    data.append([tweet.created_at, text, sentiment])\n",
    "\n",
    "# Save to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Date\", \"Tweet\", \"Sentiment\"])\n",
    "df.to_csv(\"samsung_sentiment_analysis1.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved 100 Samsung tweets with sentiment labels to samsung_sentiment_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9dc3200-7f30-4cde-86c6-1f0d9de2fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files combined and saved as 'samsung_sentiment_combined.csv' and '.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Step 1: Find all CSV files matching the pattern\n",
    "csv_files = glob.glob(\"samsung_sentiment_*.csv\")\n",
    "\n",
    "# Step 2: Read and combine all found CSV files\n",
    "if csv_files:\n",
    "    combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "    # Step 3: Save to new CSV and Excel files\n",
    "    combined_df.to_csv(\"samsung_sentiment_combined.csv\", index=False)\n",
    "    combined_df.to_excel(\"samsung_sentiment_combined.xlsx\", index=False)\n",
    "\n",
    "    print(\"✅ Files combined and saved as 'samsung_sentiment_combined.csv' and '.xlsx'\")\n",
    "else:\n",
    "    print(\"❌ No files found matching 'samsung_sentiment_*.csv'. Please check the filenames and location.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8cb590-3408-4e43-8744-cd61d5761a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment scores added and saved to 'samsung_sentiment_with_scores.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load your existing dataset\n",
    "df = pd.read_excel('samsung_sentiment_combined.xlsx')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER compound score to each tweet\n",
    "df['Score'] = df['Tweet'].apply(lambda x: sid.polarity_scores(str(x))['compound'])\n",
    "\n",
    "# Save the updated file\n",
    "df.to_excel('samsung_sentiment_with_scores.xlsx', index=False)\n",
    "\n",
    "print(\"Sentiment scores added and saved to 'samsung_sentiment_with_scores.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0628a0e-6fe0-45fe-bd70-a143d58f7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word cloud CSV generated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your existing tweets dataset\n",
    "df = pd.read_csv(\"samsung_sentiment_combined.csv\")  # or your file name\n",
    "\n",
    "# Function to clean and tokenize\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"http\\S+|@\\S+|#[A-Za-z0-9_]+\", \"\", text)  # Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\") and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Separate by sentiment\n",
    "data = []\n",
    "for sentiment in df['Sentiment'].unique():\n",
    "    subset = df[df['Sentiment'] == sentiment]\n",
    "    words = []\n",
    "    for tweet in subset['Tweet']:\n",
    "        words += preprocess(str(tweet))\n",
    "    word_freq = Counter(words)\n",
    "    for word, freq in word_freq.most_common(50):  # top 50 words per sentiment\n",
    "        data.append({'Word': word, 'Frequency': freq, 'Sentiment': sentiment})\n",
    "\n",
    "# Create DataFrame\n",
    "wordcloud_df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "wordcloud_df.to_csv(\"wordcloud_by_sentiment.csv\", index=False)\n",
    "print(\"✅ Word cloud CSV generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
